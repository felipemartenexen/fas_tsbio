{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "291e0707",
   "metadata": {},
   "source": [
    "# Pipeline Produto 1 — Base TSBio (RAW → Processado → Catálogo + Base Consolidada)\n",
    "\n",
    "Este notebook substitui os scripts CLI e roda **todo o pipeline** em ambiente Jupyter.\n",
    "\n",
    "## O que ele faz\n",
    "1. **Processa** `data/Indicadores/**.csv` → `data/Indicadores_processado_por_tema/<Categoria>/*.csv`\n",
    "2. Gera **Catálogo** → `outputs/catalogo_indicadores_tsbio.(csv|xlsx)`\n",
    "3. Gera **Base Consolidada (long/tidy)** → `outputs/base_consolidada_tsbio.csv`\n",
    "\n",
    "> **Padrão de nome de arquivo bruto**:  \n",
    "> `FONTE - TEMA - RECORTE.csv` (RECORTE é opcional)  \n",
    "> Ex.: `IBGE Censo Agro 2017 - Área média (ha) - Municípios.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc70127",
   "metadata": {},
   "source": [
    "## 0) Configuração rápida\n",
    "\n",
    "Ajuste apenas as variáveis abaixo (pastas de entrada/saída).  \n",
    "Depois rode as células na ordem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77ae3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('C:/Users/luiz.felipe/Desktop/FLP/MapiaEng/GitHub/fas_tsbio/data/Indicadores'),\n",
       " WindowsPath('C:/Users/luiz.felipe/Desktop/FLP/MapiaEng/GitHub/fas_tsbio/data/Indicadores_processado_por_tema/csv'),\n",
       " WindowsPath('C:/Users/luiz.felipe/Desktop/FLP/MapiaEng/GitHub/fas_tsbio/data/Indicadores_processado_por_tema/xlsx'),\n",
       " WindowsPath('C:/Users/luiz.felipe/Desktop/FLP/MapiaEng/GitHub/fas_tsbio/data/Indicadores_processado_por_tema/outputs'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURAÇÃO (layout atual do seu projeto)\n",
    "# =============================================================================\n",
    "\n",
    "DATA_DIR = Path(r\"C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\")\n",
    "\n",
    "# Brutos\n",
    "ROOT_RAW = DATA_DIR / \"Indicadores\"\n",
    "\n",
    "# Processados (1 arquivo por TEMA, com todos os municípios TSBio)\n",
    "OUT_PROCESSADO = DATA_DIR / \"Indicadores_processado_por_tema\"\n",
    "OUT_PROCESSADO_CSV = OUT_PROCESSADO / \"csv\"\n",
    "OUT_PROCESSADO_XLSX = OUT_PROCESSADO / \"xlsx\"\n",
    "\n",
    "# Saídas gerais do pipeline (catálogo + documentação + bases consolidadas)\n",
    "OUT_DIR = OUT_PROCESSADO / \"outputs\"\n",
    "\n",
    "# Dicionário oficial (coloque em fas_tsbio\\notebook\\)\n",
    "DICT_PATH = Path(r\"C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\notebook\\dicionario_nomes_oficial_tsbio.csv\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONTROLES (o TdR pediu CSV + Excel)\n",
    "# =============================================================================\n",
    "EXPORT_PROCESSADO_CSV = True     # gera CSV por tema (canônico)\n",
    "EXPORT_PROCESSADO_XLSX = True    # gera XLSX por tema (entrega)\n",
    "REQUIRE_FULL_COVERAGE = False    # se True, marca \"incompleto\" quando faltar município\n",
    "EXPORT_PROCESSADO_XLSX_ENGINE = \"openpyxl\"\n",
    "\n",
    "# Arquivos de saída (catálogo e documentação)\n",
    "OUT_CATALOGO_CSV = OUT_DIR / \"catalogo_indicadores_tsbio.csv\"\n",
    "OUT_CATALOGO_XLSX = OUT_DIR / \"catalogo_indicadores_tsbio.xlsx\"\n",
    "OUT_DOC_MD = OUT_DIR / \"_documentacao.md\"\n",
    "OUT_DOC_XLSX = OUT_DIR / \"_documentacao.xlsx\"\n",
    "\n",
    "# =============================================================================\n",
    "# BASE CONSOLIDADA (FULL/DASHBOARD) — recomendado PARQUET/CSV.GZ\n",
    "# =============================================================================\n",
    "OUTPUT_FORMAT = \"parquet\"   # \"parquet\" | \"csv_gz\" | \"csv\"\n",
    "ONLY_NUMERIC_ROWS = True\n",
    "DROP_REPEATED_TEXT = True\n",
    "\n",
    "GENERATE_FULL_BASE = True\n",
    "GENERATE_DASHBOARD_BASE = True\n",
    "\n",
    "OUT_BASE_FULL_PARQUET = OUT_DIR / \"base_consolidada_tsbio_full.parquet\"\n",
    "OUT_BASE_FULL_CSV_GZ  = OUT_DIR / \"base_consolidada_tsbio_full.csv.gz\"\n",
    "OUT_BASE_DASH_PARQUET = OUT_DIR / \"base_consolidada_tsbio_dashboard.parquet\"\n",
    "OUT_BASE_DASH_CSV_GZ  = OUT_DIR / \"base_consolidada_tsbio_dashboard.csv.gz\"\n",
    "\n",
    "# =============================================================================\n",
    "# DASHBOARD — seleção de indicadores\n",
    "# =============================================================================\n",
    "# Opção A (recomendada): crie/edite o arquivo curado com coluna dashboard = sim/não\n",
    "CATALOGO_CURADO_PATH = OUT_DIR / \"catalogo_indicadores_tsbio_curado.csv\"\n",
    "DASHBOARD_FLAG_COLUMN = \"dashboard\"\n",
    "DASHBOARD_INDICADOR_IDS = []   # Opção B: lista manual\n",
    "DASHBOARD_FALLBACK_MAX = 80\n",
    "\n",
    "# =============================================================================\n",
    "# PREPARO\n",
    "# =============================================================================\n",
    "OUT_PROCESSADO_CSV.mkdir(parents=True, exist_ok=True)\n",
    "OUT_PROCESSADO_XLSX.mkdir(parents=True, exist_ok=True)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert ROOT_RAW.exists(), f\"ROOT_RAW não encontrado: {ROOT_RAW}\"\n",
    "assert DICT_PATH.exists(), f\"DICT_PATH não encontrado: {DICT_PATH}\"\n",
    "\n",
    "ROOT_RAW, OUT_PROCESSADO_CSV, OUT_PROCESSADO_XLSX, OUT_DIR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ca0eb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5cedb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIG TSBio (6 territórios)\n",
    "# =============================================================================\n",
    "\n",
    "TSBIO = [\n",
    "    {\"territorio_id\": 1, \"territorio_nome\": \"Altamira\", \"CD_MUN\": [\"1500602\",\"1500859\",\"1501725\",\"1504455\",\"1505486\",\"1507805\",\"1508159\",\"1508357\"]},\n",
    "    {\"territorio_id\": 2, \"territorio_nome\": \"Macapá\", \"CD_MUN\": [\"1600212\",\"1600303\",\"1600253\",\"1600238\",\"1600535\",\"1600600\",\"1600154\",\"1600055\"]},\n",
    "    {\"territorio_id\": 3, \"territorio_nome\": \"Portel\", \"CD_MUN\": [\"1503101\",\"1504505\",\"1505809\",\"1501105\"]},\n",
    "    {\"territorio_id\": 4, \"territorio_nome\": \"Juruá-Tefé\", \"CD_MUN\": [\"1301654\",\"1301803\",\"1301407\",\"1301506\",\"1301951\",\"1301001\",\"1304203\",\"1302207\",\"1304260\",\"1300029\"]},\n",
    "    {\"territorio_id\": 5, \"territorio_nome\": \"Rio Branco-Brasiléia\", \"CD_MUN\": [\"1200401\",\"1200708\",\"1200252\",\"1200104\",\"1200054\",\"1200138\",\"1200807\",\"1200450\",\"1200013\",\"1200385\",\"1200179\"]},\n",
    "    {\"territorio_id\": 6, \"territorio_nome\": \"Salgado-Bragantino\", \"CD_MUN\": [\"1508209\",\"1508035\",\"1507961\",\"1507474\",\"1507466\",\"1507409\",\"1507102\",\"1506906\",\"1506609\",\"1506203\",\"1506112\",\"1506104\",\"1505601\",\"1505007\",\"1504406\",\"1504307\",\"1504109\",\"1503200\",\"1502905\",\"1502608\",\"1502202\",\"1501709\",\"1501600\",\"1500909\"]},\n",
    "]\n",
    "\n",
    "CATEGORIA_TO_DIMENSAO = {\n",
    "    \"População\": \"socioeconômica\",\n",
    "    \"Educação\": \"socioeconômica\",\n",
    "    \"Domicílios\": \"socioeconômica\",\n",
    "    \"Trabalho e Renda\": \"socioeconômica\",\n",
    "    \"Mercado de Trabalho\": \"socioeconômica\",\n",
    "    \"Economia\": \"socioeconômica\",\n",
    "    \"Religião\": \"socioeconômica\",\n",
    "    \"Indígenas\": \"socioeconômica\",\n",
    "    \"Quilombola\": \"socioeconômica\",\n",
    "    \"Índices\": \"socioeconômica\",\n",
    "    \"Desmatamento\": \"ambiental\",\n",
    "    \"Uso e Cobertura do Solo\": \"ambiental\",\n",
    "    \"Fundiário\": \"ambiental\",\n",
    "    \"Agropecuária\": \"produtiva\",\n",
    "    \"Entorno Domicílios\": \"infraestrutura\",\n",
    "    \"Assistência Social\": \"políticas_públicas\",\n",
    "    \"Vulnerabilidade\": \"vulnerabilidades\",\n",
    "    \"Favelas e Comunidades Urbanas\": \"vulnerabilidades\",\n",
    "}\n",
    "\n",
    "# Separadores\n",
    "CSV_SEP_DEFAULT = \";\"\n",
    "SEPS_CANDIDATES = [\";\", \",\", \"\\t\"]\n",
    "OUT_SEP = \";\"\n",
    "OUT_ENCODING = \"utf-8-sig\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3359fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Utilidades — normalização / dicionário / CSV\n",
    "# =============================================================================\n",
    "\n",
    "def safe_filename(name: str) -> str:\n",
    "    name = re.sub(r'[\\\\/:*?\"<>|]+', \"_\", (name or \"\").strip())\n",
    "    name = re.sub(r\"\\s+\", \" \", name).strip()\n",
    "    return name or \"_\"\n",
    "\n",
    "def slugify(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join(ch for ch in s if not unicodedata.combining(ch))\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def zfill_mun(x) -> str:\n",
    "    x = re.sub(r\"\\D+\", \"\", str(x))\n",
    "    return x.zfill(7) if x else \"\"\n",
    "\n",
    "def parse_parts_from_filename(filename: str) -> Tuple[str, str, str]:\n",
    "    \"\"\"Extrai (fonte, tema, recorte) via separador seguro ' - '.\"\"\"\n",
    "    name = filename[:-4] if filename.lower().endswith(\".csv\") else filename\n",
    "    parts = [p.strip() for p in name.split(\" - \") if p.strip()]\n",
    "    fonte = parts[0] if len(parts) >= 1 else \"\"\n",
    "    tema = parts[1] if len(parts) >= 2 else (parts[0] if parts else \"\")\n",
    "    recorte = parts[2] if len(parts) >= 3 else \"\"\n",
    "    return fonte, tema, recorte\n",
    "\n",
    "def build_indicador_id(categoria: str, fonte: str, tema: str, recorte: str = \"\") -> str:\n",
    "    base = f\"{slugify(categoria)}__{slugify(fonte)}__{slugify(tema)}\"\n",
    "    if recorte and slugify(recorte) not in (\"municipios\", \"municípios\"):\n",
    "        base = f\"{base}__{slugify(recorte)}\"\n",
    "    return base\n",
    "\n",
    "def detect_csv_sep(path: Path, encoding: str) -> str:\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=encoding, errors=\"replace\") as f:\n",
    "            header = f.readline()\n",
    "            if header.lower().startswith(\"sep=\"):\n",
    "                header = f.readline()\n",
    "    except Exception:\n",
    "        return CSV_SEP_DEFAULT\n",
    "\n",
    "    if not header:\n",
    "        return CSV_SEP_DEFAULT\n",
    "\n",
    "    counts = {s: header.count(s) for s in SEPS_CANDIDATES}\n",
    "    best = max(counts, key=counts.get)\n",
    "    return best if counts[best] > 0 else CSV_SEP_DEFAULT\n",
    "\n",
    "def read_csv_local(path: Path) -> pd.DataFrame:\n",
    "    last_err = None\n",
    "    for enc in (\"utf-8-sig\", \"latin1\"):\n",
    "        try:\n",
    "            sep = detect_csv_sep(path, enc)\n",
    "            return pd.read_csv(path, sep=sep, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            continue\n",
    "    raise last_err\n",
    "\n",
    "def load_dictionary(path: Path) -> Dict[str, str]:\n",
    "    \"\"\"Carrega dicionário oficial: normalized_synonym -> canonical.\"\"\"\n",
    "    if not path.exists():\n",
    "        return {}\n",
    "    df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "    syn_map: Dict[str, str] = {}\n",
    "    for _, r in df.iterrows():\n",
    "        canonical = str(r[\"canonical\"]).strip()\n",
    "        syns = str(r.get(\"synonyms\", \"\") or \"\").split(\"|\")\n",
    "        for s in syns + [str(r.get(\"label_pt\", \"\") or \"\")]:\n",
    "            ss = slugify(s)\n",
    "            if ss:\n",
    "                syn_map[ss] = canonical\n",
    "    return syn_map\n",
    "\n",
    "_UNIT_SUFFIX_MAP = {\n",
    "    \"%\": \"perc\",\n",
    "    \"percent\": \"perc\",\n",
    "    \"porcentagem\": \"perc\",\n",
    "    \"ha\": \"ha\",\n",
    "    \"hectare\": \"ha\",\n",
    "    \"hectares\": \"ha\",\n",
    "    \"km2\": \"km2\",\n",
    "    \"km²\": \"km2\",\n",
    "    \"m2\": \"m2\",\n",
    "    \"m²\": \"m2\",\n",
    "    \"r$\": \"rs\",\n",
    "    \"rs\": \"rs\",\n",
    "    \"reais\": \"rs\",\n",
    "    \"pessoas\": \"pessoas\",\n",
    "    \"pessoa\": \"pessoas\",\n",
    "}\n",
    "\n",
    "def _unit_to_suffix(unit_raw: str) -> str:\n",
    "    u = (unit_raw or \"\").strip().lower()\n",
    "    if not u:\n",
    "        return \"\"\n",
    "    u = u.replace(\" \", \"\")\n",
    "    u = u.replace(\"R$\", \"r$\").replace(\"²\", \"2\")\n",
    "    if \"/\" in u:\n",
    "        left, right = u.split(\"/\", 1)\n",
    "        left_s = _UNIT_SUFFIX_MAP.get(left, slugify(left))\n",
    "        right_s = _UNIT_SUFFIX_MAP.get(right, slugify(right))\n",
    "        return f\"{left_s}_por_{right_s}\".strip(\"_\")\n",
    "    return _UNIT_SUFFIX_MAP.get(u, slugify(u))\n",
    "\n",
    "_UNIT_PATTERN = re.compile(r\"^(.*?)[\\s]*[\\(\\[\\{]([^\\)\\]\\}]+)[\\)\\]\\}]\\s*$\")\n",
    "\n",
    "def normalize_column_name(col: str, syn_map: Dict[str, str]) -> str:\n",
    "    \"\"\"\n",
    "    1) Se bater no dicionário oficial -> canonical\n",
    "    2) Senão: snake_case + unidade no sufixo se houver (ex.: '(ha)' -> '_ha')\n",
    "    \"\"\"\n",
    "    col = (col or \"\").strip()\n",
    "    if not col:\n",
    "        return col\n",
    "\n",
    "    s = slugify(col)\n",
    "    if s in syn_map:\n",
    "        return syn_map[s]\n",
    "\n",
    "    m = _UNIT_PATTERN.match(col)\n",
    "    unit_suffix = \"\"\n",
    "    base = col\n",
    "    if m:\n",
    "        base = m.group(1).strip()\n",
    "        unit_suffix = _unit_to_suffix(m.group(2).strip())\n",
    "\n",
    "    base_slug = slugify(base)\n",
    "    if unit_suffix and not base_slug.endswith(\"_\" + unit_suffix):\n",
    "        base_slug = f\"{base_slug}_{unit_suffix}\"\n",
    "    return base_slug or s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a096582d",
   "metadata": {},
   "source": [
    "## 1) Processar brutos → processados\n",
    "\n",
    "Gera um CSV por indicador, já com colunas padronizadas e metadados adicionados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0152cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_indicadores(\n",
    "    root_raw: Path,\n",
    "    out_csv_root: Path,\n",
    "    out_xlsx_root: Path,\n",
    "    dict_path: Path,\n",
    "    export_csv: bool = True,\n",
    "    export_xlsx: bool = True,\n",
    "    require_full_coverage: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Processa brutos -> 1 arquivo por TEMA (com todos os municípios).\n",
    "\n",
    "    Regras principais:\n",
    "    - Fonte = antes do 1º ' - '\n",
    "    - Tema  = texto ENTRE o 1º e o 2º ' - '  (a \"parte do meio\")\n",
    "    - Recorte (se existir) vira METADADO (coluna), mas NÃO vira arquivo separado.\n",
    "\n",
    "    Saída:\n",
    "    - out_csv_root/<Categoria>/<Tema>.csv\n",
    "    - out_xlsx_root/<Categoria>/<Tema>.xlsx\n",
    "    \"\"\"\n",
    "\n",
    "    syn_map = load_dictionary(dict_path)\n",
    "\n",
    "    # mapa TSBio\n",
    "    expected_muns = set()\n",
    "    mun_to_tsbio: Dict[str, Tuple[int, str]] = {}\n",
    "    for t in TSBIO:\n",
    "        tid = int(t[\"territorio_id\"])\n",
    "        tname = str(t[\"territorio_nome\"])\n",
    "        for m in t[\"CD_MUN\"]:\n",
    "            mm = zfill_mun(m)\n",
    "            expected_muns.add(mm)\n",
    "            mun_to_tsbio[mm] = (tid, tname)\n",
    "\n",
    "    csv_files = sorted(root_raw.rglob(\"*.csv\"))\n",
    "    print(f\"CSV brutos encontrados: {len(csv_files)} em {root_raw}\")\n",
    "\n",
    "    # bucket por categoria+fonte+tema (recorte não separa arquivo)\n",
    "    buckets: Dict[Tuple[str, str, str], List[pd.DataFrame]] = defaultdict(list)\n",
    "\n",
    "    # para evitar colisão de nomes quando houver mesmo tema com fontes diferentes\n",
    "    fontes_por_cat_tema: Dict[Tuple[str, str], set] = defaultdict(set)\n",
    "\n",
    "    errors = []\n",
    "    missing_cod_mun = []\n",
    "\n",
    "    for p in tqdm(csv_files, desc=\"Lendo CSVs brutos\"):\n",
    "        try:\n",
    "            rel = p.relative_to(root_raw)\n",
    "            categoria = rel.parts[0] if len(rel.parts) >= 2 else \"(raiz)\"\n",
    "        except Exception:\n",
    "            categoria = \"(raiz)\"\n",
    "\n",
    "        fonte, tema, recorte = parse_parts_from_filename(p.name)\n",
    "        fontes_por_cat_tema[(categoria, tema)].add(fonte)\n",
    "\n",
    "        # indicador_id ignora recorte por definição (1 arquivo por tema)\n",
    "        indicador_id = build_indicador_id(categoria, fonte, tema, recorte=\"\")\n",
    "\n",
    "        try:\n",
    "            df = read_csv_local(p)\n",
    "        except Exception as e:\n",
    "            errors.append((str(p), str(e)))\n",
    "            continue\n",
    "\n",
    "        # padroniza colunas via dicionário oficial\n",
    "        col_map = {c: normalize_column_name(c, syn_map) for c in df.columns}\n",
    "        df = df.rename(columns=col_map)\n",
    "\n",
    "        # fallback extra para casos raros\n",
    "        if \"cod_municipio\" not in df.columns:\n",
    "            # tenta localizar algo parecido\n",
    "            possible = [c for c in df.columns if c.endswith(\"municipio\") or \"mun\" in c]\n",
    "            if possible:\n",
    "                # não renomeia automaticamente para evitar erro silencioso\n",
    "                pass\n",
    "            missing_cod_mun.append(str(p))\n",
    "            continue\n",
    "\n",
    "        # padroniza código do município\n",
    "        df[\"cod_municipio\"] = df[\"cod_municipio\"].apply(zfill_mun)\n",
    "        df = df[df[\"cod_municipio\"].astype(str).str.len() > 0].copy()\n",
    "\n",
    "        # filtro TSBio\n",
    "        df = df[df[\"cod_municipio\"].isin(expected_muns)].copy()\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        # metadados\n",
    "        df[\"categoria\"] = categoria\n",
    "        df[\"fonte\"] = fonte\n",
    "        df[\"tema\"] = tema\n",
    "        df[\"recorte\"] = recorte\n",
    "        df[\"indicador_id\"] = indicador_id\n",
    "        df[\"arquivo_origem\"] = p.name\n",
    "\n",
    "        # mapeamento TSBio\n",
    "        df[\"territorio_id\"] = df[\"cod_municipio\"].map(lambda x: mun_to_tsbio.get(x, (None, None))[0])\n",
    "        df[\"territorio_nome\"] = df[\"cod_municipio\"].map(lambda x: mun_to_tsbio.get(x, (None, None))[1])\n",
    "        df = df[df[\"territorio_id\"].notna()].copy()\n",
    "\n",
    "        buckets[(categoria, fonte, tema)].append(df)\n",
    "\n",
    "    report = []\n",
    "\n",
    "    for (categoria, fonte, tema), dfs in buckets.items():\n",
    "        big = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        present = set(big[\"cod_municipio\"].dropna().unique())\n",
    "        missing = sorted(list(expected_muns - present))\n",
    "        status = \"ok\" if (not require_full_coverage or not missing) else \"incompleto\"\n",
    "\n",
    "        # pastas por categoria\n",
    "        cat_dir_csv = out_csv_root / safe_filename(categoria)\n",
    "        cat_dir_xlsx = out_xlsx_root / safe_filename(categoria)\n",
    "        cat_dir_csv.mkdir(parents=True, exist_ok=True)\n",
    "        cat_dir_xlsx.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # nome do arquivo: 1 arquivo por tema (mas evita colisões se houver o mesmo tema com fontes diferentes)\n",
    "        if len(fontes_por_cat_tema[(categoria, tema)]) > 1:\n",
    "            out_base = safe_filename(f\"{tema} - {fonte}\")\n",
    "        else:\n",
    "            out_base = safe_filename(tema)\n",
    "\n",
    "        out_csv_path = cat_dir_csv / f\"{out_base}.csv\"\n",
    "        out_xlsx_path = cat_dir_xlsx / f\"{out_base}.xlsx\"\n",
    "\n",
    "        # ordenação de colunas (metadados primeiro)\n",
    "        first_cols = [\n",
    "            \"indicador_id\",\n",
    "            \"territorio_id\", \"territorio_nome\",\n",
    "            \"categoria\", \"fonte\", \"tema\", \"recorte\",\n",
    "            \"cod_municipio\", \"municipio_nome\", \"sigla_uf\",\n",
    "            \"ano\", \"mes\", \"classe\", \"observacao\",\n",
    "            \"arquivo_origem\",\n",
    "        ]\n",
    "        cols = [c for c in first_cols if c in big.columns] + [c for c in big.columns if c not in first_cols]\n",
    "        big = big[cols]\n",
    "\n",
    "        # export\n",
    "        if export_csv:\n",
    "            big.to_csv(out_csv_path, index=False, sep=OUT_SEP, encoding=OUT_ENCODING)\n",
    "\n",
    "        excel_path_info = None\n",
    "        if export_xlsx:\n",
    "            try:\n",
    "                big.to_excel(out_xlsx_path, index=False, engine=\"openpyxl\", sheet_name=\"dados\")\n",
    "                excel_path_info = str(out_xlsx_path)\n",
    "            except Exception as e:\n",
    "                excel_path_info = f\"erro: {e}\"\n",
    "                print(f\"⚠️ Falha ao exportar Excel ({out_xlsx_path}): {e}\")\n",
    "\n",
    "        report.append({\n",
    "            \"categoria\": categoria,\n",
    "            \"fonte\": fonte,\n",
    "            \"tema\": tema,\n",
    "            \"status\": status,\n",
    "            \"arquivo_csv\": str(out_csv_path) if export_csv else None,\n",
    "            \"arquivo_excel\": excel_path_info,\n",
    "            \"linhas\": len(big),\n",
    "            \"n_presentes\": len(present),\n",
    "            \"n_esperados\": len(expected_muns),\n",
    "            \"faltando_cod_municipio\": \",\".join(missing) if missing else \"\",\n",
    "            \"n_colunas\": len(big.columns),\n",
    "        })\n",
    "\n",
    "    rep_df = pd.DataFrame(report).sort_values([\"status\", \"categoria\", \"fonte\", \"tema\"])\n",
    "    rep_path = out_csv_root.parent / \"_relatorio_validacao.csv\" if out_csv_root.name == \"csv\" else out_csv_root / \"_relatorio_validacao.csv\"\n",
    "    rep_df.to_csv(rep_path, index=False, encoding=OUT_ENCODING)\n",
    "\n",
    "    if errors:\n",
    "        err_path = out_csv_root.parent / \"_erros_leitura.csv\" if out_csv_root.name == \"csv\" else out_csv_root / \"_erros_leitura.csv\"\n",
    "        pd.DataFrame(errors, columns=[\"arquivo\", \"erro\"]).to_csv(err_path, index=False, encoding=OUT_ENCODING)\n",
    "\n",
    "    if missing_cod_mun:\n",
    "        miss_path = out_csv_root.parent / \"_sem_coluna_cod_municipio.csv\" if out_csv_root.name == \"csv\" else out_csv_root / \"_sem_coluna_cod_municipio.csv\"\n",
    "        pd.DataFrame({\"arquivo\": missing_cod_mun}).to_csv(miss_path, index=False, encoding=OUT_ENCODING)\n",
    "\n",
    "    print(f\"✅ Processamento concluído. Relatório: {rep_path}\")\n",
    "    return rep_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39eddaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV brutos encontrados: 9773 em C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lendo CSVs brutos: 100%|██████████| 9773/9773 [03:53<00:00, 41.84it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processamento concluído. Relatório: C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\_relatorio_validacao.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "categoria",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "fonte",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tema",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "status",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arquivo_csv",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "arquivo_excel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "linhas",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_presentes",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "n_esperados",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "faltando_cod_municipio",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "n_colunas",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "763ba94b-b198-496f-85e6-efdcc8837b08",
       "rows": [
        [
         "0",
         "Agropecuária",
         "IBGE PAM 2024",
         "Lavoura Permanente",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Lavoura Permanente.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Lavoura Permanente.xlsx",
         "125970",
         "65",
         "65",
         "",
         "19"
        ],
        [
         "1",
         "Agropecuária",
         "IBGE PAM 2024",
         "Lavoura Temporaria",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Lavoura Temporaria.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Lavoura Temporaria.xlsx",
         "109395",
         "65",
         "65",
         "",
         "19"
        ],
        [
         "2",
         "Agropecuária",
         "IBGE PEVS 2024",
         "Produção Extração Vegetal",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Produção Extração Vegetal.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Produção Extração Vegetal.xlsx",
         "10471",
         "65",
         "65",
         "",
         "15"
        ],
        [
         "3",
         "Agropecuária",
         "IBGE PEVS 2024",
         "Produção Silvicultura",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Produção Silvicultura.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Produção Silvicultura.xlsx",
         "69",
         "5",
         "65",
         "1200013,1200054,1200104,1200138,1200179,1200252,1200385,1200401,1200450,1200708,1200807,1300029,1301001,1301407,1301506,1301654,1301803,1301951,1302207,1304203,1304260,1500859,1500909,1501105,1501600,1501709,1501725,1502202,1502608,1502905,1503101,1503200,1504109,1504307,1504406,1504455,1504505,1505007,1505486,1505601,1505809,1506104,1506112,1506203,1506609,1506906,1507102,1507409,1507466,1507474,1507805,1507961,1508035,1508159,1508209,1508357,1600055,1600154,1600212,1600600",
         "17"
        ],
        [
         "4",
         "Agropecuária",
         "IBGE PPM 2024",
         "Efetivo Rebanhos",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Efetivo Rebanhos.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Efetivo Rebanhos.xlsx",
         "15838",
         "65",
         "65",
         "",
         "13"
        ],
        [
         "5",
         "Agropecuária",
         "IBGE PPM 2024",
         "Producão Aquicultura",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Producão Aquicultura.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Producão Aquicultura.xlsx",
         "1892",
         "54",
         "65",
         "1302207,1501105,1502608,1503101,1504505,1505007,1507466,1600212,1600253,1600535,1600600",
         "14"
        ],
        [
         "6",
         "Agropecuária",
         "IBGE PPM 2024",
         "Produção Origem Animal",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Produção Origem Animal.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Produção Origem Animal.xlsx",
         "7171",
         "65",
         "65",
         "",
         "15"
        ],
        [
         "7",
         "Agropecuária",
         "IBGE PPM 2024",
         "Produção Pecuária",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Produção Pecuária.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Produção Pecuária.xlsx",
         "2630",
         "65",
         "65",
         "",
         "13"
        ],
        [
         "8",
         "Agropecuária",
         "PRONAF",
         "Cobertura do PRONAF",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Agropecuária\\Cobertura do PRONAF.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Agropecuária\\Cobertura do PRONAF.xlsx",
         "65",
         "65",
         "65",
         "",
         "14"
        ],
        [
         "9",
         "Assistência Social",
         "Observatório Cadastro Único",
         "Cadastro Único",
         "ok",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\csv\\Assistência Social\\Cadastro Único.csv",
         "C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitHub\\fas_tsbio\\data\\Indicadores_processado_por_tema\\xlsx\\Assistência Social\\Cadastro Único.xlsx",
         "65",
         "65",
         "65",
         "",
         "20"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categoria</th>\n",
       "      <th>fonte</th>\n",
       "      <th>tema</th>\n",
       "      <th>status</th>\n",
       "      <th>arquivo_csv</th>\n",
       "      <th>arquivo_excel</th>\n",
       "      <th>linhas</th>\n",
       "      <th>n_presentes</th>\n",
       "      <th>n_esperados</th>\n",
       "      <th>faltando_cod_municipio</th>\n",
       "      <th>n_colunas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PAM 2024</td>\n",
       "      <td>Lavoura Permanente</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>125970</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PAM 2024</td>\n",
       "      <td>Lavoura Temporaria</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>109395</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PEVS 2024</td>\n",
       "      <td>Produção Extração Vegetal</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>10471</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PEVS 2024</td>\n",
       "      <td>Produção Silvicultura</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>1200013,1200054,1200104,1200138,1200179,120025...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PPM 2024</td>\n",
       "      <td>Efetivo Rebanhos</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>15838</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PPM 2024</td>\n",
       "      <td>Producão Aquicultura</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>1892</td>\n",
       "      <td>54</td>\n",
       "      <td>65</td>\n",
       "      <td>1302207,1501105,1502608,1503101,1504505,150500...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PPM 2024</td>\n",
       "      <td>Produção Origem Animal</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>7171</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>IBGE PPM 2024</td>\n",
       "      <td>Produção Pecuária</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>2630</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Agropecuária</td>\n",
       "      <td>PRONAF</td>\n",
       "      <td>Cobertura do PRONAF</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Assistência Social</td>\n",
       "      <td>Observatório Cadastro Único</td>\n",
       "      <td>Cadastro Único</td>\n",
       "      <td>ok</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            categoria                        fonte                       tema  \\\n",
       "0        Agropecuária                IBGE PAM 2024         Lavoura Permanente   \n",
       "1        Agropecuária                IBGE PAM 2024         Lavoura Temporaria   \n",
       "2        Agropecuária               IBGE PEVS 2024  Produção Extração Vegetal   \n",
       "3        Agropecuária               IBGE PEVS 2024      Produção Silvicultura   \n",
       "4        Agropecuária                IBGE PPM 2024           Efetivo Rebanhos   \n",
       "5        Agropecuária                IBGE PPM 2024       Producão Aquicultura   \n",
       "6        Agropecuária                IBGE PPM 2024     Produção Origem Animal   \n",
       "7        Agropecuária                IBGE PPM 2024          Produção Pecuária   \n",
       "8        Agropecuária                       PRONAF        Cobertura do PRONAF   \n",
       "9  Assistência Social  Observatório Cadastro Único             Cadastro Único   \n",
       "\n",
       "  status                                        arquivo_csv  \\\n",
       "0     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "1     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "2     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "3     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "4     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "5     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "6     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "7     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "8     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "9     ok  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   \n",
       "\n",
       "                                       arquivo_excel  linhas  n_presentes  \\\n",
       "0  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...  125970           65   \n",
       "1  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...  109395           65   \n",
       "2  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   10471           65   \n",
       "3  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...      69            5   \n",
       "4  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...   15838           65   \n",
       "5  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...    1892           54   \n",
       "6  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...    7171           65   \n",
       "7  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...    2630           65   \n",
       "8  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...      65           65   \n",
       "9  C:\\Users\\luiz.felipe\\Desktop\\FLP\\MapiaEng\\GitH...      65           65   \n",
       "\n",
       "   n_esperados                             faltando_cod_municipio  n_colunas  \n",
       "0           65                                                            19  \n",
       "1           65                                                            19  \n",
       "2           65                                                            15  \n",
       "3           65  1200013,1200054,1200104,1200138,1200179,120025...         17  \n",
       "4           65                                                            13  \n",
       "5           65  1302207,1501105,1502608,1503101,1504505,150500...         14  \n",
       "6           65                                                            15  \n",
       "7           65                                                            13  \n",
       "8           65                                                            14  \n",
       "9           65                                                            20  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rep_df = processar_indicadores(\n",
    "    root_raw=ROOT_RAW,\n",
    "    out_csv_root=OUT_PROCESSADO_CSV,\n",
    "    out_xlsx_root=OUT_PROCESSADO_XLSX,\n",
    "    dict_path=DICT_PATH,\n",
    "    export_csv=EXPORT_PROCESSADO_CSV,\n",
    "    export_xlsx=EXPORT_PROCESSADO_XLSX,\n",
    "    require_full_coverage=REQUIRE_FULL_COVERAGE,\n",
    ")\n",
    "rep_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40b95d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiz.felipe\\AppData\\Local\\Temp\\ipykernel_26812\\3287141967.py:48: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for (categoria,), gcat in doc_df.groupby([\"categoria\"]):\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 80\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m -\u001b[39m\u001b[38;5;124m\"\u001b[39m, out_xlsx)\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_df, cols_df\n\u001b[1;32m---> 80\u001b[0m doc_df, cols_df \u001b[38;5;241m=\u001b[39m \u001b[43mgerar_documentacao\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrep_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_DOC_MD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUT_DOC_XLSX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m doc_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[11], line 48\u001b[0m, in \u001b[0;36mgerar_documentacao\u001b[1;34m(rep_df, out_md, out_xlsx)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# ---- MD ----\u001b[39;00m\n\u001b[0;32m     47\u001b[0m lines \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (categoria,), gcat \u001b[38;5;129;01min\u001b[39;00m doc_df\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategoria\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m     49\u001b[0m     lines\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategoria\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, rr \u001b[38;5;129;01min\u001b[39;00m gcat\u001b[38;5;241m.\u001b[39miterrows():\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 1)"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def gerar_documentacao(\n",
    "    rep_df: pd.DataFrame,\n",
    "    out_md: Path,\n",
    "    out_xlsx: Path,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Gera documentação (MD + XLSX) a partir do relatório do processamento.\"\"\"\n",
    "\n",
    "    rows = []\n",
    "    col_rows = []\n",
    "\n",
    "    for _, r in rep_df.iterrows():\n",
    "        csv_path = Path(r[\"arquivo_csv\"]) if pd.notna(r.get(\"arquivo_csv\")) and r.get(\"arquivo_csv\") else None\n",
    "        xlsx_path = r.get(\"arquivo_excel\") if pd.notna(r.get(\"arquivo_excel\")) else None\n",
    "\n",
    "        cols = []\n",
    "        if csv_path and csv_path.exists():\n",
    "            try:\n",
    "                cols = list(pd.read_csv(csv_path, sep=OUT_SEP, encoding=OUT_ENCODING, nrows=0).columns)\n",
    "            except Exception:\n",
    "                cols = []\n",
    "        rows.append({\n",
    "            \"categoria\": r.get(\"categoria\", \"\"),\n",
    "            \"fonte\": r.get(\"fonte\", \"\"),\n",
    "            \"tema\": r.get(\"tema\", \"\"),\n",
    "            \"status\": r.get(\"status\", \"\"),\n",
    "            \"arquivo_csv\": str(csv_path) if csv_path else \"\",\n",
    "            \"arquivo_excel\": xlsx_path if isinstance(xlsx_path, str) else \"\",\n",
    "            \"linhas\": r.get(\"linhas\", \"\"),\n",
    "            \"n_colunas\": len(cols) if cols else r.get(\"n_colunas\", \"\"),\n",
    "            \"colunas\": \"; \".join(cols),\n",
    "        })\n",
    "\n",
    "        for c in cols:\n",
    "            col_rows.append({\n",
    "                \"categoria\": r.get(\"categoria\", \"\"),\n",
    "                \"fonte\": r.get(\"fonte\", \"\"),\n",
    "                \"tema\": r.get(\"tema\", \"\"),\n",
    "                \"coluna\": c,\n",
    "            })\n",
    "\n",
    "    doc_df = pd.DataFrame(rows).sort_values([\"categoria\", \"fonte\", \"tema\"])\n",
    "    cols_df = pd.DataFrame(col_rows).sort_values([\"categoria\", \"fonte\", \"tema\", \"coluna\"])\n",
    "\n",
    "    # ---- MD ----\n",
    "    lines = []\n",
    "    for (categoria,), gcat in doc_df.groupby([\"categoria\"]):\n",
    "        lines.append(f\"# {categoria}\")\n",
    "        for _, rr in gcat.iterrows():\n",
    "            lines.append(f\"## {rr['tema']}\")\n",
    "            if rr.get(\"fonte\"):\n",
    "                lines.append(f\"- **Fonte:** {rr['fonte']}\")\n",
    "            if rr.get(\"arquivo_csv\"):\n",
    "                lines.append(f\"- **CSV:** {rr['arquivo_csv']}\")\n",
    "            if rr.get(\"arquivo_excel\"):\n",
    "                lines.append(f\"- **XLSX:** {rr['arquivo_excel']}\")\n",
    "            if rr.get(\"linhas\") != \"\":\n",
    "                lines.append(f\"- **Linhas:** {rr['linhas']}\")\n",
    "            lines.append(\"- **Colunas:**\")\n",
    "            cols = [c.strip() for c in str(rr.get(\"colunas\",\"\")).split(\";\") if c.strip()]\n",
    "            for c in cols:\n",
    "                lines.append(f\"  - {c}\")\n",
    "            lines.append(\"\")\n",
    "        lines.append(\"\")\n",
    "\n",
    "    out_md.write_text(\"\\n\".join(lines).strip() + \"\\n\", encoding=\"utf-8\")\n",
    "\n",
    "    # ---- XLSX ----\n",
    "    with pd.ExcelWriter(out_xlsx, engine=\"openpyxl\") as writer:\n",
    "        doc_df.to_excel(writer, index=False, sheet_name=\"arquivos\")\n",
    "        cols_df.to_excel(writer, index=False, sheet_name=\"colunas\")\n",
    "\n",
    "    print(\"✅ Documentação gerada:\")\n",
    "    print(\" -\", out_md)\n",
    "    print(\" -\", out_xlsx)\n",
    "\n",
    "    return doc_df, cols_df\n",
    "\n",
    "doc_df, cols_df = gerar_documentacao(rep_df, OUT_DOC_MD, OUT_DOC_XLSX)\n",
    "doc_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5720d85",
   "metadata": {},
   "source": [
    "## 2) Gerar base consolidada (long/tidy)\n",
    "\n",
    "Transforma os CSVs processados em uma base única no formato long:\n",
    "- `variavel`, `valor`, `valor_num`, `unidade`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b3b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIT_SUFFIX_TO_UNIT = {\n",
    "    \"perc\": \"%\",\n",
    "    \"ha\": \"ha\",\n",
    "    \"km2\": \"km²\",\n",
    "    \"m2\": \"m²\",\n",
    "    \"rs\": \"R$\",\n",
    "    \"pessoas\": \"pessoas\",\n",
    "}\n",
    "\n",
    "def infer_unidade_from_variavel(variavel: str) -> str:\n",
    "    if not variavel:\n",
    "        return \"\"\n",
    "    v = str(variavel).strip().lower()\n",
    "    suf = v.split(\"_\")[-1]\n",
    "    return UNIT_SUFFIX_TO_UNIT.get(suf, \"\")\n",
    "\n",
    "def identificar_colunas_valor(df: pd.DataFrame) -> List[str]:\n",
    "    excluir = {\n",
    "        \"indicador_id\", \"territorio_id\", \"territorio_nome\", \"categoria\",\n",
    "        \"fonte\", \"tema\", \"recorte\", \"dimensao_tdr\",\n",
    "        \"cod_municipio\", \"municipio_nome\", \"sigla_uf\",\n",
    "        \"ano\", \"mes\", \"classe\", \"observacao\", \"arquivo_origem\",\n",
    "    }\n",
    "    cols_valor = []\n",
    "    for col in df.columns:\n",
    "        if col in excluir:\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            cols_valor.append(col)\n",
    "            continue\n",
    "        if df[col].dtype == \"object\":\n",
    "            s = df[col].astype(str)\n",
    "            numeric = pd.to_numeric(\n",
    "                s.str.replace(\".\", \"\", regex=False)\n",
    "                 .str.replace(\",\", \".\", regex=False)\n",
    "                 .str.replace(\"%\", \"\", regex=False)\n",
    "                 .str.replace(\" \", \"\", regex=False),\n",
    "                errors=\"coerce\",\n",
    "            )\n",
    "            if numeric.notna().mean() > 0.5:\n",
    "                cols_valor.append(col)\n",
    "    return cols_valor\n",
    "\n",
    "def transformar_para_long(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    id_cols = [c for c in [\n",
    "        \"territorio_id\",\"territorio_nome\",\"cod_municipio\",\"municipio_nome\",\"sigla_uf\",\n",
    "        \"ano\",\"mes\",\"indicador_id\",\"tema\",\"categoria\",\"dimensao_tdr\",\"fonte\",\n",
    "        \"recorte\",\"classe\",\"observacao\",\"arquivo_origem\"\n",
    "    ] if c in df.columns]\n",
    "\n",
    "    value_cols = identificar_colunas_valor(df)\n",
    "    if not value_cols:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    melted = pd.melt(df, id_vars=id_cols, value_vars=value_cols, var_name=\"variavel\", value_name=\"valor\")\n",
    "\n",
    "    s = melted[\"valor\"].astype(str)\n",
    "    melted[\"valor_num\"] = pd.to_numeric(\n",
    "        s.str.replace(\".\", \"\", regex=False)\n",
    "         .str.replace(\",\", \".\", regex=False)\n",
    "         .str.replace(\"%\", \"\", regex=False)\n",
    "         .str.replace(\" \", \"\", regex=False),\n",
    "        errors=\"coerce\",\n",
    "    )\n",
    "\n",
    "    melted[\"unidade\"] = melted[\"variavel\"].map(infer_unidade_from_variavel)\n",
    "\n",
    "    order = [\n",
    "        \"territorio_id\",\"territorio_nome\",\"cod_municipio\",\"municipio_nome\",\"sigla_uf\",\n",
    "        \"ano\",\"mes\",\n",
    "        \"indicador_id\",\"tema\",\"categoria\",\"dimensao_tdr\",\"fonte\",\"recorte\",\n",
    "        \"variavel\",\"valor\",\"valor_num\",\"unidade\",\n",
    "        \"classe\",\"observacao\",\"arquivo_origem\",\n",
    "    ]\n",
    "    cols = [c for c in order if c in melted.columns] + [c for c in melted.columns if c not in order]\n",
    "    return melted[cols]\n",
    "\n",
    "def _aplicar_controles_tamanho(base: pd.DataFrame) -> pd.DataFrame:\n",
    "    # mantém apenas linhas numéricas (melhor para dashboard)\n",
    "    if \"ONLY_NUMERIC_ROWS\" in globals() and ONLY_NUMERIC_ROWS and \"valor_num\" in base.columns:\n",
    "        base = base[base[\"valor_num\"].notna()].copy()\n",
    "\n",
    "    # tabela fato enxuta (puxa metadados do catálogo depois via indicador_id)\n",
    "    if \"DROP_REPEATED_TEXT\" in globals() and DROP_REPEATED_TEXT:\n",
    "        drop_cols = [\n",
    "            \"valor\",  # redundante se valor_num existe\n",
    "            \"tema\", \"categoria\", \"dimensao_tdr\", \"fonte\", \"recorte\", \"arquivo_origem\",\n",
    "            \"territorio_nome\", \"municipio_nome\", \"sigla_uf\",\n",
    "            \"observacao\",\n",
    "        ]\n",
    "        base = base.drop(columns=drop_cols, errors=\"ignore\")\n",
    "    return base\n",
    "\n",
    "def _salvar_base(base: pd.DataFrame, kind: str = \"FULL\"):\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    fmt = OUTPUT_FORMAT if \"OUTPUT_FORMAT\" in globals() else \"parquet\"\n",
    "\n",
    "    if kind.upper() == \"DASHBOARD\":\n",
    "        parquet_path = OUT_BASE_DASH_PARQUET\n",
    "        csv_gz_path = OUT_BASE_DASH_CSV_GZ\n",
    "        csv_path = OUT_DIR / \"base_consolidada_tsbio_dashboard.csv\"\n",
    "    else:\n",
    "        parquet_path = OUT_BASE_FULL_PARQUET\n",
    "        csv_gz_path = OUT_BASE_FULL_CSV_GZ\n",
    "        csv_path = OUT_DIR / \"base_consolidada_tsbio_full.csv\"\n",
    "\n",
    "    if fmt == \"parquet\":\n",
    "        try:\n",
    "            base.to_parquet(parquet_path, index=False)\n",
    "            print(f\"✅ Base {kind} (PARQUET) salva em: {parquet_path} (linhas={len(base)})\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Não consegui salvar PARQUET para {kind} (provável falta de pyarrow/fastparquet).\")\n",
    "            print(\"   Salvando em CSV.GZ como fallback.\")\n",
    "            base.to_csv(csv_gz_path, index=False, sep=OUT_SEP, encoding=OUT_ENCODING, compression=\"gzip\")\n",
    "            print(f\"✅ Base {kind} (CSV.GZ) salva em: {csv_gz_path} (linhas={len(base)})\")\n",
    "            return\n",
    "\n",
    "    if fmt == \"csv_gz\":\n",
    "        base.to_csv(csv_gz_path, index=False, sep=OUT_SEP, encoding=OUT_ENCODING, compression=\"gzip\")\n",
    "        print(f\"✅ Base {kind} (CSV.GZ) salva em: {csv_gz_path} (linhas={len(base)})\")\n",
    "    else:\n",
    "        base.to_csv(csv_path, index=False, sep=OUT_SEP, encoding=OUT_ENCODING)\n",
    "        print(f\"✅ Base {kind} (CSV) salva em: {csv_path} (linhas={len(base)})\")\n",
    "\n",
    "def gerar_base_consolidada(root_processado: Path, filter_indicador_ids: list | None = None) -> pd.DataFrame:\n",
    "    files = sorted(root_processado.rglob(\"*.csv\"))\n",
    "    print(f\"Processados encontrados: {len(files)} em {root_processado}\")\n",
    "\n",
    "    out_parts = []\n",
    "    for p in tqdm(files, desc=\"Consolidando\"):\n",
    "        try:\n",
    "            df = read_csv_local(p)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        # filtro de indicadores (se solicitado)\n",
    "        if filter_indicador_ids is not None and \"indicador_id\" in df.columns:\n",
    "            # indicador_id costuma ser constante no arquivo\n",
    "            iid = str(df[\"indicador_id\"].dropna().iloc[0]) if df[\"indicador_id\"].notna().any() else \"\"\n",
    "            if iid and iid not in set(filter_indicador_ids):\n",
    "                continue\n",
    "\n",
    "        df_long = transformar_para_long(df)\n",
    "        if not df_long.empty:\n",
    "            out_parts.append(df_long)\n",
    "\n",
    "    if not out_parts:\n",
    "        raise RuntimeError(\"Nenhum dado consolidado (sem colunas de valor detectadas ou filtro muito restritivo).\")\n",
    "\n",
    "    base = pd.concat(out_parts, ignore_index=True)\n",
    "    base = _aplicar_controles_tamanho(base)\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb71261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 2.1) Gera base FULL (arquivo/consulta) ======\n",
    "if GENERATE_FULL_BASE:\n",
    "    base_full = gerar_base_consolidada(OUT_PROCESSADO_CSV, filter_indicador_ids=None)\n",
    "    _salvar_base(base_full, kind=\"FULL\")\n",
    "else:\n",
    "    base_full = None\n",
    "    print(\"ℹ️ GENERATE_FULL_BASE=False (pulando base FULL)\")\n",
    "\n",
    "# ====== 2.2) Gera base DASHBOARD (subconjunto) ======\n",
    "if GENERATE_DASHBOARD_BASE:\n",
    "    # dashboard_ids foi definido na célula 3.1 (seleção)\n",
    "    base_dash = gerar_base_consolidada(OUT_PROCESSADO_CSV, filter_indicador_ids=dashboard_ids)\n",
    "    _salvar_base(base_dash, kind=\"DASHBOARD\")\n",
    "else:\n",
    "    base_dash = None\n",
    "    print(\"ℹ️ GENERATE_DASHBOARD_BASE=False (pulando base DASHBOARD)\")\n",
    "\n",
    "# Mostra amostra da base do dashboard\n",
    "(base_dash if base_dash is not None else base_full).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fc9219",
   "metadata": {},
   "source": [
    "## 3) Gerar catálogo de indicadores (CSV + XLSX)\n",
    "\n",
    "O catálogo é uma tabela “dimensão” que complementa a base consolidada e ajuda no Looker Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03719f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "def identificar_colunas_valor_cols(cols: List[str]) -> List[str]:\n",
    "    excluir = {\n",
    "        \"indicador_id\",\"territorio_id\",\"territorio_nome\",\"categoria\",\"fonte\",\"tema\",\"recorte\",\"dimensao_tdr\",\n",
    "        \"cod_municipio\",\"municipio_nome\",\"sigla_uf\",\"ano\",\"mes\",\"classe\",\"observacao\",\"arquivo_origem\",\n",
    "    }\n",
    "    return [c for c in cols if c not in excluir]\n",
    "\n",
    "def inferir_unidade(variaveis: List[str]) -> str:\n",
    "    units = set()\n",
    "    for v in variaveis:\n",
    "        suf = str(v).split(\"_\")[-1].lower()\n",
    "        u = UNIT_SUFFIX_TO_UNIT.get(suf, \"\")\n",
    "        if u:\n",
    "            units.add(u)\n",
    "    if len(units) == 1:\n",
    "        return list(units)[0]\n",
    "    if len(units) > 1:\n",
    "        return \"multiplas\"\n",
    "    return \"\"\n",
    "\n",
    "def inferir_periodo(path: Path) -> str:\n",
    "    try:\n",
    "        df = read_csv_local(path)\n",
    "        if \"ano\" not in df.columns:\n",
    "            return \"\"\n",
    "        anos = pd.to_numeric(df[\"ano\"], errors=\"coerce\").dropna()\n",
    "        if anos.empty:\n",
    "            return \"\"\n",
    "        return f\"{int(anos.min())}-{int(anos.max())}\"\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def inferir_frequencia_por_fonte(fonte: str) -> str:\n",
    "    f = (fonte or \"\").lower()\n",
    "    if \"censo\" in f:\n",
    "        return \"decenal\"\n",
    "    if \"mapbiomas\" in f or \"inpe\" in f:\n",
    "        return \"anual\"\n",
    "    if \"rais\" in f or \"caged\" in f:\n",
    "        return \"mensal/anual\"\n",
    "    return \"a definir\"\n",
    "\n",
    "def criar_excel(path_xlsx: Path, df: pd.DataFrame, sheet_name: str = \"catalogo\"):\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.title = sheet_name\n",
    "\n",
    "    header_fill = PatternFill(\"solid\", fgColor=\"1F4E79\")\n",
    "    header_font = Font(color=\"FFFFFF\", bold=True)\n",
    "    center = Alignment(horizontal=\"center\", vertical=\"center\", wrap_text=True)\n",
    "    left = Alignment(horizontal=\"left\", vertical=\"top\", wrap_text=True)\n",
    "    thin = Side(style=\"thin\", color=\"D9D9D9\")\n",
    "    border = Border(left=thin, right=thin, top=thin, bottom=thin)\n",
    "\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(df, index=False, header=True), start=1):\n",
    "        ws.append(row)\n",
    "        if r_idx == 1:\n",
    "            for c_idx in range(1, len(row) + 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx)\n",
    "                cell.fill = header_fill\n",
    "                cell.font = header_font\n",
    "                cell.alignment = center\n",
    "                cell.border = border\n",
    "        else:\n",
    "            for c_idx in range(1, len(row) + 1):\n",
    "                cell = ws.cell(row=r_idx, column=c_idx)\n",
    "                cell.alignment = left\n",
    "                cell.border = border\n",
    "\n",
    "    for col in ws.columns:\n",
    "        max_len = 0\n",
    "        col_letter = col[0].column_letter\n",
    "        for cell in col:\n",
    "            try:\n",
    "                max_len = max(max_len, len(str(cell.value)))\n",
    "            except Exception:\n",
    "                pass\n",
    "        ws.column_dimensions[col_letter].width = min(max(12, max_len + 2), 55)\n",
    "\n",
    "    wb.save(path_xlsx)\n",
    "\n",
    "def gerar_catalogo(root_processado: Path, out_csv: Path, out_xlsx: Path) -> pd.DataFrame:\n",
    "    files = sorted(root_processado.rglob(\"*.csv\"))\n",
    "    registros = []\n",
    "\n",
    "    for p in tqdm(files, desc=\"Catalogando\"):\n",
    "        categoria = p.parent.name\n",
    "\n",
    "        try:\n",
    "            df_head = read_csv_local(p).head(5)\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        indicador_id = str(df_head[\"indicador_id\"].dropna().iloc[0]) if \"indicador_id\" in df_head.columns and df_head[\"indicador_id\"].notna().any() else \"\"\n",
    "        tema = str(df_head[\"tema\"].dropna().iloc[0]) if \"tema\" in df_head.columns and df_head[\"tema\"].notna().any() else \"\"\n",
    "        fonte = str(df_head[\"fonte\"].dropna().iloc[0]) if \"fonte\" in df_head.columns and df_head[\"fonte\"].notna().any() else \"\"\n",
    "        recorte = str(df_head[\"recorte\"].dropna().iloc[0]) if \"recorte\" in df_head.columns and df_head[\"recorte\"].notna().any() else \"\"\n",
    "        dimensao_tdr = str(df_head[\"dimensao_tdr\"].dropna().iloc[0]) if \"dimensao_tdr\" in df_head.columns and df_head[\"dimensao_tdr\"].notna().any() else CATEGORIA_TO_DIMENSAO.get(categoria, \"outros\")\n",
    "\n",
    "        variaveis = identificar_colunas_valor_cols(list(df_head.columns))\n",
    "        unidade = inferir_unidade(variaveis)\n",
    "        periodo = inferir_periodo(p)\n",
    "        freq = inferir_frequencia_por_fonte(fonte)\n",
    "\n",
    "        registros.append({\n",
    "            \"indicador_id\": indicador_id,\n",
    "            \"categoria\": categoria,\n",
    "            \"tema\": tema,\n",
    "            \"fonte\": fonte,\n",
    "            \"recorte\": recorte,\n",
    "            \"dimensao_tdr\": dimensao_tdr,\n",
    "            \"unidade\": unidade,\n",
    "            \"nivel_territorial\": \"municipio\",\n",
    "            \"periodo\": periodo,\n",
    "            \"frequencia_update\": freq,\n",
    "            \"anexo_ii\": \"não\",  # marcar depois (ou criar regras)\n",
    "            \"arquivo_csv\": str(p),\n",
    "            \"variaveis\": \", \".join(variaveis[:80]),\n",
    "            \"n_variaveis\": len(variaveis),\n",
    "        })\n",
    "\n",
    "    cat = pd.DataFrame(registros).sort_values([\"categoria\", \"fonte\", \"tema\"])\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cat.to_csv(out_csv, index=False, encoding=OUT_ENCODING)\n",
    "    criar_excel(out_xlsx, cat)\n",
    "\n",
    "    print(f\"✅ Catálogo salvo em: {out_csv}\")\n",
    "    print(f\"✅ Catálogo (XLSX) salvo em: {out_xlsx}\")\n",
    "    return cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99e918",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogo = gerar_catalogo(OUT_PROCESSADO_CSV, OUT_CATALOGO_CSV, OUT_CATALOGO_XLSX)\n",
    "catalogo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0e1243",
   "metadata": {},
   "source": [
    "## 3.1) Selecionar quais indicadores entram no dashboard\n",
    "\n",
    "Você tem 2 jeitos:\n",
    "\n",
    "### Opção A (recomendada)\n",
    "1. Rode a geração do catálogo (célula anterior)  \n",
    "2. Copie `outputs/catalogo_indicadores_tsbio.csv` para `outputs/catalogo_indicadores_tsbio_curado.csv`  \n",
    "3. No arquivo curado, adicione/edite a coluna `dashboard` e marque `sim` nos indicadores que quer no Looker.\n",
    "\n",
    "### Opção B\n",
    "Preencha a lista `DASHBOARD_INDICADOR_IDS` na célula de configuração.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truthy(x) -> bool:\n",
    "    if x is None:\n",
    "        return False\n",
    "    s = str(x).strip().lower()\n",
    "    return s in {\"1\", \"true\", \"t\", \"yes\", \"y\", \"sim\", \"s\"}\n",
    "\n",
    "def selecionar_indicadores_para_dashboard(catalogo_df: pd.DataFrame) -> list:\n",
    "    # 1) Se existir catálogo curado, usa ele\n",
    "    df = catalogo_df.copy()\n",
    "    if \"CATALOGO_CURADO_PATH\" in globals() and CATALOGO_CURADO_PATH.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(CATALOGO_CURADO_PATH, encoding=OUT_ENCODING)\n",
    "            print(f\"✅ Usando catálogo curado: {CATALOGO_CURADO_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(\"⚠️ Não consegui ler o catálogo curado, vou usar o catálogo gerado.\")\n",
    "            print(e)\n",
    "\n",
    "    # 2) Se tiver coluna de flag (dashboard), usa\n",
    "    flag_col = DASHBOARD_FLAG_COLUMN if \"DASHBOARD_FLAG_COLUMN\" in globals() else \"dashboard\"\n",
    "    if flag_col in df.columns:\n",
    "        sel = df[df[flag_col].map(_truthy)]\n",
    "        ids = sel[\"indicador_id\"].dropna().astype(str).unique().tolist()\n",
    "        if ids:\n",
    "            print(f\"✅ Indicadores marcados para dashboard via coluna '{flag_col}': {len(ids)}\")\n",
    "            return ids\n",
    "\n",
    "    # 3) Se tiver lista manual preenchida, usa\n",
    "    if \"DASHBOARD_INDICADOR_IDS\" in globals() and isinstance(DASHBOARD_INDICADOR_IDS, list) and len(DASHBOARD_INDICADOR_IDS) > 0:\n",
    "        ids = [str(x).strip() for x in DASHBOARD_INDICADOR_IDS if str(x).strip()]\n",
    "        print(f\"✅ Indicadores para dashboard via lista manual: {len(ids)}\")\n",
    "        return ids\n",
    "\n",
    "    # 4) Fallback: tenta pegar anexo_ii == sim (se existir) ou os primeiros N\n",
    "    if \"anexo_ii\" in df.columns:\n",
    "        sel = df[df[\"anexo_ii\"].astype(str).str.lower().isin({\"sim\", \"s\", \"1\", \"true\"})]\n",
    "        ids = sel[\"indicador_id\"].dropna().astype(str).unique().tolist()\n",
    "        if ids:\n",
    "            print(f\"ℹ️ Fallback: usando anexo_ii=sim ({len(ids)})\")\n",
    "            return ids\n",
    "\n",
    "    nmax = DASHBOARD_FALLBACK_MAX if \"DASHBOARD_FALLBACK_MAX\" in globals() else 80\n",
    "    ids = df[\"indicador_id\"].dropna().astype(str).unique().tolist()[:nmax]\n",
    "    print(f\"⚠️ Nenhum indicador marcado. Fallback: primeiros {len(ids)} indicadores (max={nmax}).\")\n",
    "    return ids\n",
    "\n",
    "dashboard_ids = selecionar_indicadores_para_dashboard(catalogo)\n",
    "pd.Series(dashboard_ids, name=\"indicador_id\").to_csv(OUT_DIR / \"indicadores_dashboard_ids.csv\", index=False, encoding=OUT_ENCODING)\n",
    "print(f\"✅ Lista salva em: {OUT_DIR / 'indicadores_dashboard_ids.csv'}\")\n",
    "len(dashboard_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb59c8e",
   "metadata": {},
   "source": [
    "## 4) Checagens rápidas (sanidade)\n",
    "\n",
    "Você pode usar estas checagens antes de publicar no Looker Studio.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952b2f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Base consolidada:\")\n",
    "print(\" - linhas:\", len(base))\n",
    "print(\" - indicadores:\", base[\"indicador_id\"].nunique() if \"indicador_id\" in base.columns else None)\n",
    "print(\" - municipios:\", base[\"cod_municipio\"].nunique() if \"cod_municipio\" in base.columns else None)\n",
    "print(\" - anos:\", (int(base[\"ano\"].min()), int(base[\"ano\"].max())) if \"ano\" in base.columns and pd.api.types.is_numeric_dtype(base[\"ano\"]) else None)\n",
    "\n",
    "print(\"\\nCatálogo:\")\n",
    "print(\" - linhas:\", len(catalogo))\n",
    "print(\" - indicadores:\", catalogo[\"indicador_id\"].nunique() if \"indicador_id\" in catalogo.columns else None)\n",
    "\n",
    "# Top 10 indicadores com mais linhas\n",
    "if \"indicador_id\" in base.columns:\n",
    "    base.groupby(\"indicador_id\").size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da0dd",
   "metadata": {},
   "source": [
    "## 5) Publicar no Looker Studio (recomendação)\n",
    "\n",
    "- Fonte 1: `outputs/base_consolidada_tsbio.csv`\n",
    "- Fonte 2: `outputs/catalogo_indicadores_tsbio.xlsx` (ou CSV)\n",
    "- Relacionar por: `indicador_id`\n",
    "\n",
    "> Dica: Use filtros por `territorio_nome`, `ano`, `dimensao_tdr` e `fonte`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientificProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
